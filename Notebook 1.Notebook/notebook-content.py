# Fabric notebook source

# METADATA ********************

# META {
# META   "kernel_info": {
# META     "name": "synapse_pyspark"
# META   },
# META   "dependencies": {
# META     "lakehouse": {
# META       "default_lakehouse": "88897e89-1437-4418-9ee0-f6417e7e9447",
# META       "default_lakehouse_name": "lh_tpch02",
# META       "default_lakehouse_workspace_id": "d5c3bf3a-e87e-45b7-b746-ac561f7416c1",
# META       "known_lakehouses": [
# META         {
# META           "id": "88897e89-1437-4418-9ee0-f6417e7e9447"
# META         }
# META       ]
# META     }
# META   }
# META }

# CELL ********************

# Welcome to your new notebook
# Type here in the cell editor to add code!



df_customer = spark.sql("SELECT * FROM lh_tpch02.customer")
# display(df_customer)

df_lineitem = spark.sql("SELECT * FROM lh_tpch02.lineitem")
# display(df_lineitem)

df_orders = spark.sql("SELECT * FROM lh_tpch02.orders")
# display(df_orders)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

## Clean up Orders ##

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(df_orders):
    # Filter rows based on column: 'orderstatus'
    df_orders = df_orders.filter(df_orders['orderstatus'] == "F")
    # Drop columns: 'orderstatus', 'shippriority' and 2 other columns
    df_orders = df_orders.drop('orderstatus', 'shippriority', 'clearkdate', 'orderpriority')
    return df_orders

df_orders_clean = clean_data(df_orders)
# display(df_orders_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

## Clean up Customer ##

# Code generated by Data Wrangler for PySpark DataFrame

def clean_data(df_customer):
    # Drop columns: 'acctbal', 'mktsegment'
    df_customer = df_customer.drop('acctbal', 'mktsegment')
    return df_customer

df_customer_clean = clean_data(df_customer)
# display(df_customer_clean)

# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************

## Join customer and orders ##

df_cust_order = df_customer_clean.join(df_orders_clean, 'custkey')
# display(df_cust_order)


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }

# CELL ********************


# With Spark SQL, Please run the query onto the lakehouse which is from the same workspace as the current default lakehouse.

df = spark.sql("SELECT * FROM lh_nhs_open_data.Clinics")


# METADATA ********************

# META {
# META   "language": "python",
# META   "language_group": "synapse_pyspark"
# META }
